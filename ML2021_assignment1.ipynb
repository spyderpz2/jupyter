{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "*Part of the course:\n",
    "Machine Learning (code: INFOB3ML), fall 2021, Utrecht University*\n",
    "\n",
    "Total points: 9 (+ 1 for free)\n",
    "\n",
    "Submit one ipynb file per pair.\n",
    "\n",
    "**Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Regularisation\n",
    "In the final assignment of Introduction to Machine Learning, you performed several simulation experiments with linear regression, examining the impact of different target functions, different hypothesis classes, different noise levels, and different amounts of training data on the performance of linear regression. (If you didn't do that assignment, don't worry: everything you need to know about it is also in chapter 1 of the Rogers & Girolami book.)\n",
    "\n",
    "In this assignment, we continue such simulation experiments, this time in order to investigate a new topic: regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Data generation\n",
    "Every datapoint $(x,t)$ will be sampled randomly, with both $x$ and $t$ in $\\mathbb{R}$. (Note that the book from Introduction to Machine Learning called the output $y$, while this course's book calls it $t$.) The input $x$ is normally distributed, with expected value = 0 and variance = 1. \n",
    "\n",
    "Once we have our input $x$, we generate our output $t$ according to $t = \\sin(x) + \\epsilon$, where $\\epsilon$ is normally distributed with expected value = 0 and variance = $\\sigma^2$.\n",
    "All the random numbers should be generated **independently** from one another.\n",
    "\n",
    "### Regression\n",
    "\n",
    "We'll implement *regularised* regression, adding a penalty $\\lambda \\mathbf{w}^T \\mathbf{w}$ to our training loss. Linear regression with this form of regularisation penalty is also called *ridge regression*. We are going to try out different values of $\\lambda$.\n",
    "\n",
    "We'll use regression with order five polynomials like in the book. This means that for each weight vector $\\mathbf{w}$, our hypothesis is of the form\n",
    "$$f(x; \\mathbf{w}) = \\sum_{i=0}^5 w_i x^i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Code\n",
    "\n",
    "To make it clear what your code is supposed to do and how it should be formatted, we provide you general schema for each to-be-written function. Some functions come with additional hints about useful in-built functions or procedural details. You might write the function's body differently than the hints suggests. That's totally fine as long as the function works as it supposed to work.\n",
    "Please **remove the provided hints** (formatted as comments) because they are redundunt after the code is written.\n",
    "\n",
    "Use numpy for functionalities involving vectors and matrices. [Here is a handy guide (in notebook form) that deals with numpy arrays, matrices and number generation.](https://github.com/ageron/handson-ml/blob/master/tools_numpy.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 1** (1 point)\n",
    "\n",
    "Write a function `generate_data` you can use to generate a dataset and outputs the pair of vectors `(x,t)`, accepting parameters $N$ and $\\sigma^2$. Be sure to check if your normal-distribution-generator needs $\\sigma$ (standard deviation) or $\\sigma^2$ (variance) as input parameter. But `x` and `t` should be 1-dimensional numpy arrays, i.e. their shape should be `(N,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# functions you might use: np.ones, np.random.randn, np.hstack, np.matmul (or @), np.dot,\n",
    "\n",
    "# Likely mistakes:\n",
    "# - noise generated with the wrong variance (e.g. math.sqrt needed but missing)\n",
    "\n",
    "def generate_data(N, sigma_squared):\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ██████████ TEST ██████████\n",
    "# (These \"TEST\" blocks can help you quickly check if there's something obviously wrong with the code you wrote.)\n",
    "# Setting a seed helps to make the data generation deterministic for comparison reasons\n",
    "np.random.seed(0) \n",
    "toy_xs, toy_t = generate_data(3, 0.1)\n",
    "# Check the shapes of the output arrays are as specified above:\n",
    "print(toy_xs.shape)\n",
    "print(toy_xs)\n",
    "print(toy_t.shape)\n",
    "print(toy_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 2** (1 point)\n",
    "\n",
    "Write a function `compute_X_matrix` that takes a numpy array of `x`s as produced by your code above, and returns the matrix `X` needed by linear regression with 5th-order polynomials. (See equation (1.18) on page 28 of the book for what this matrix should look like.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_X_matrix(x_scalars):\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ██████████ TEST ██████████\n",
    "toy_X = compute_X_matrix(toy_xs)\n",
    "print(toy_xs)\n",
    "print(toy_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 3** (1 point)\n",
    "\n",
    "Write code that fits a regularised linear regression hypothesis to training data, in other words, a function that computes our $\\hat{\\mathbf{w}}$. Use numpy to carry out the necessary matrix operations to find an analytic solution; don't use linear regression functionality from Python packages for machine learning. In other words, compute $\\hat{\\mathbf{w}}$ according to the equation (1.21) on page 36 of the book. Give your function a parameter `lamb` which tells it the value of $\\lambda$.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(X, t, lamb):\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ██████████ TEST ██████████\n",
    "# this test uses values for toy_X and toy_t printed above\n",
    "# You migh need to define these values by hand\n",
    "# if your \"generate_data\" returns different data\n",
    "print(toy_X)\n",
    "print(toy_t)\n",
    "print(fit_ridge(toy_X, toy_t, 0.01))\n",
    "print(fit_ridge(toy_X, toy_t, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared loss over the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code (which you don't need to change) evaluates a learned linear regression function $\\hat{\\mathbf{w}}$ with respect to the data $\\mathbf{X}, \\mathbf{t}$ using the squared error loss. This Python function can be used to compute training, validation or test loss for $\\hat{\\mathbf{w}}$, depending of the kind of data passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(w, X, t):\n",
    "    N, k = X.shape\n",
    "    t_hat = X @ w\n",
    "    t_error = t_hat - t\n",
    "    sum_of_squared_errors = t_error.T @ t_error\n",
    "    loss = sum_of_squared_errors / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to do some experiments and look at the results.\n",
    "\n",
    "First, the function provided below will plot the target function $\\sin(x)$, a regression function, and the training data all in one plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regression_result(xs_train, t_train, w, include_target_function=True):\n",
    "    xs_plot = np.linspace(-3, 3, 101)\n",
    "    X_plot = compute_X_matrix(xs_plot)\n",
    "    if include_target_function:\n",
    "        plt.plot(xs_plot, np.sin(xs_plot), c='black') # target \n",
    "    plt.scatter(xs_train, t_train, c='blue', marker=\".\")\n",
    "    plt.plot(xs_plot, X_plot @ w, c='red')\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 4** (1 point)\n",
    "\n",
    "Generate a dataset of $N=10$ data points with noise level $\\sigma^2 = 0.1$. (You'll use this dataset in assignments/questions 4 through 6). For the values of $\\lambda$ provided in the code below, fit a regularised regression curve to the data and compute the loss. Display the results in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = 10 ** np.linspace(-5, 5, 11)\n",
    "losses = np.zeros_like(lambdas)\n",
    "\n",
    "np.random.seed(0) # To get the same data each time you run this code\n",
    "\n",
    "# Your code here\n",
    "    \n",
    "plt.semilogx(lambdas, losses, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5** (1 point) Your plot should show that as $\\lambda$ gets larger, the loss also gets larger. Explain why this is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment/Question 6** (1 point)\n",
    "\n",
    "Using the same data, answer the following questions:\n",
    "\n",
    "For what values of $\\lambda$ do you clearly see overfitting? For what values of $\\lambda$ do you see underfitting? To support your answer, include plots for some values of $\\lambda$, and point out what features of those plots tell you that over-/underfitting is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a good value of $\\lambda$, a variety of techniques exist. One that obviously does *not* work is to look at the training loss as a function of $\\lambda$ (like you plotted above): that would always suggest to make $\\lambda$ as small as possible! A versatile technique that you've already seen in Introduction to Machine Learning (or in section 1.5 of the book) is **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 7** (1 point)\n",
    "\n",
    "Write some code to do the following: sample a new dataset of $N = 50$ data points and $\\sigma^2=0.1$. (You'll use this dataset for all the remaining assignments and questions.) Write a function that, given data and value of $\\lambda$, computes the leave-one-out cross-validation (LOOCV) loss, as explained in section 1.5.2 of the book. Then make a plot similar to what we did before for the training loss, but this time displaying the LOOCV loss as a function of $\\lambda$.\n",
    "\n",
    "Note that the third argument, `fitting_function`, should be the name of a function that `LOOCV` can call to compute `w`. If `fit_ridge` is passed, your previously written function will be used. But later, you'll call it with a different fitting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOOCV(X, t, fitting_function, lamb):\n",
    "    N, k = X.shape\n",
    "    sum_of_losses = 0.0\n",
    "    for leave_out in range(N):\n",
    "        # Your code here\n",
    "        pass\n",
    "        \n",
    "    return sum_of_losses / N\n",
    "\n",
    "# Your code here to sample a larger dataset, and to make the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8** (0.5 points): What value of $\\lambda$ does LOOCV point you to? Look at a plot of the resulting regression function. Does it look reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the book mentions, when doing regularisation, using the squares of $\\mathbf{w}$ as a penalty is just one of many possibilities. It has the advantage of having an analytical solution. But other options exist that may have other advantages, and while they may not be analytically computable, still there exist efficient algorithms for working with them. A particularly popular one is to use the sum of absolute values of $\\mathbf{w}$ as a penalty: we will find the $\\mathbf{w}$ that minimizes\n",
    "$$\\mathcal{L}' = \\mathcal{L} + \\lambda \\sum_i \\lvert w_i \\rvert.$$\n",
    "This is called the 'lasso' (which is an acronym for 'least absolute shrinkage and selection operator', but of course most people just remember the acronym)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no direct formula for computing the $\\mathbf{w}$ that minimizes $\\mathcal{L}'$. The next alternative would be to use (stochastic) gradient descent. Unfortunately, that also doesn't work very nicely here, because as a function of $\\mathbf{w}$, $\\mathcal{L}'$ is not differentiable wherever $\\mathbf{w}$ has at least one entry equal to zero. But variants of gradient descent have been developed that can deal with this problem (such as [proximal gradient descent](https://en.wikipedia.org/wiki/Proximal_gradient_method)), and implementations are readily available. The fitting function provided below uses such an implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def fit_lasso(X, t, lamb):\n",
    "    clf = Lasso(lamb, fit_intercept=False, max_iter=100000)\n",
    "    clf.fit(X, t)\n",
    "    return clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 9** (0.5 point) Again plot the LOOCV losses as a function of $\\lambda$, but this time for lasso regression instead of ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important property of lasso regularisation is its tendency to make some weigths exactly equal to 0. (Well, mathematically that's true, but you should never rely on things being *exactly* equal when a numerical algorithm is involved. Instead, check whether the difference between them is very small, say less than `1e-9`.)\n",
    "\n",
    "**Assignment 10** (0.5 points)\n",
    "\n",
    "What is the smallest $\\lambda$ in `lambdas` for which you observe this happening for some $w_i$? For that $\\lambda$, make a plot where $w_0$ varies along the horizontal axis. On the vertical axis, plot the regularised loss $\\mathcal{L}'$ of the weight vector, with all entries other than $w_i$ kept equal to the optimal lasso solution. Choose the range of $w_i$-values small enough that you see a nondifferentiability in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11** (0.5 points): Use this graph to explain why lasso regression has a tendency to make some weights equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember: Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**<br/>\n",
    "**Also remove our hints/comments from your code**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
